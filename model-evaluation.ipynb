{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"models/nn-predictors/first_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabularies/vocabulary_small.pkl\", 'rb') as file:\n",
    "    vocabulary = pickle.load(file)\n",
    "\n",
    "with open(\"models/word2vecs/the_smallest.pkl\", 'rb') as file:\n",
    "    word2vec = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Returns the first not belonging token in the vocabulary\n",
    "\"\"\"\n",
    "def belong_to_vocabulary(tokens: list, vocabulary: list):\n",
    "    for token in tokens:\n",
    "        if not token in vocabulary:\n",
    "            return token\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens: list, word2vec):\n",
    "    embedded_tokens = np.array([])\n",
    "    \n",
    "    for token in tokens:\n",
    "        embedded_tokens = np.append(embedded_tokens, word2vec.wv.get_vector(token))\n",
    "\n",
    "    return embedded_tokens.reshape(1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "marvelous\n"
     ]
    }
   ],
   "source": [
    "# quick test\n",
    "\n",
    "print(belong_to_vocabulary(['a', 'story', 'the'], vocabulary))\n",
    "print(belong_to_vocabulary(['a', 'story', 'marvelous', 'the'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 180ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.8113226e-03, 2.9834694e-01, 7.8284580e-01, ..., 1.1835092e-03,\n",
       "        2.2432350e-06, 1.3455294e-01]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.randn(1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = ['a', 'story', 'about']\n",
    "test_tokens = ['the', 'main', 'character']\n",
    "test_tokens = ['my', 'feelings', 'were']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "not_belonging = belong_to_vocabulary(test_tokens, vocabulary)\n",
    "\n",
    "if not_belonging is None:\n",
    "    y_pred = model.predict(vectorize(test_tokens, word2vec))\n",
    "else:\n",
    "    print(\"ERROR: Word \\'{}\\' does not belong to the vocabulary\".format(not_belonging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_word = vocabulary[np.argmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proposed_word = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices = np.argsort(y_pred).reshape(len(vocabulary))[-num_proposed_word:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([185,  74,   2,   8,  27], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/reviews/neg/1_1.txt\")\n",
    "sample_review = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>False</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>[story, of, a, man, who, has, unnatural, feeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>False</td>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>[airport, starts, as, a, brand, new, luxury, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>False</td>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>[this, film, lacked, something, i, couldnt, pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>False</td>\n",
       "      <td>sorry everyone,,, i know this is supposed to b...</td>\n",
       "      <td>[sorry, everyone, ,, ,, ,, i, know, this, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>[when, i, was, little, my, parents, took, me, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>B movie at best. Sound effects are pretty good...</td>\n",
       "      <td>False</td>\n",
       "      <td>b movie at best, sound effects are pretty good...</td>\n",
       "      <td>[b, movie, at, best, ,, sound, effects, are, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>I chose to see this movie because it got a goo...</td>\n",
       "      <td>False</td>\n",
       "      <td>i chose to see this movie because it got a goo...</td>\n",
       "      <td>[i, chose, to, see, this, movie, because, it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Oh Dear Lord, How on Earth was any part of thi...</td>\n",
       "      <td>False</td>\n",
       "      <td>oh dear lord, how on earth was any part of thi...</td>\n",
       "      <td>[oh, dear, lord, ,, how, on, earth, was, any, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>This is not a film you can really analyse sepa...</td>\n",
       "      <td>False</td>\n",
       "      <td>this is not a film you can really analyse sepa...</td>\n",
       "      <td>[this, is, not, a, film, you, can, really, ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>\"Raising Victor Vargas\" is one of those light,...</td>\n",
       "      <td>False</td>\n",
       "      <td>raising victor vargas is one of those light, f...</td>\n",
       "      <td>[raising, victor, vargas, is, one, of, those, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  positive  \\\n",
       "0    Story of a man who has unnatural feelings for ...     False   \n",
       "1    Airport '77 starts as a brand new luxury 747 p...     False   \n",
       "2    This film lacked something I couldn't put my f...     False   \n",
       "3    Sorry everyone,,, I know this is supposed to b...     False   \n",
       "4    When I was little my parents took me along to ...     False   \n",
       "..                                                 ...       ...   \n",
       "495  B movie at best. Sound effects are pretty good...     False   \n",
       "496  I chose to see this movie because it got a goo...     False   \n",
       "497  Oh Dear Lord, How on Earth was any part of thi...     False   \n",
       "498  This is not a film you can really analyse sepa...     False   \n",
       "499  \"Raising Victor Vargas\" is one of those light,...     False   \n",
       "\n",
       "                                          preprocessed  \\\n",
       "0    story of a man who has unnatural feelings for ...   \n",
       "1    airport  starts as a brand new luxury  plane i...   \n",
       "2    this film lacked something i couldnt put my fi...   \n",
       "3    sorry everyone,,, i know this is supposed to b...   \n",
       "4    when i was little my parents took me along to ...   \n",
       "..                                                 ...   \n",
       "495  b movie at best, sound effects are pretty good...   \n",
       "496  i chose to see this movie because it got a goo...   \n",
       "497  oh dear lord, how on earth was any part of thi...   \n",
       "498  this is not a film you can really analyse sepa...   \n",
       "499  raising victor vargas is one of those light, f...   \n",
       "\n",
       "                                             tokenized  \n",
       "0    [story, of, a, man, who, has, unnatural, feeli...  \n",
       "1    [airport, starts, as, a, brand, new, luxury, p...  \n",
       "2    [this, film, lacked, something, i, couldnt, pu...  \n",
       "3    [sorry, everyone, ,, ,, ,, i, know, this, is, ...  \n",
       "4    [when, i, was, little, my, parents, took, me, ...  \n",
       "..                                                 ...  \n",
       "495  [b, movie, at, best, ,, sound, effects, are, p...  \n",
       "496  [i, chose, to, see, this, movie, because, it, ...  \n",
       "497  [oh, dear, lord, ,, how, on, earth, was, any, ...  \n",
       "498  [this, is, not, a, film, you, can, really, ana...  \n",
       "499  [raising, victor, vargas, is, one, of, those, ...  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"data/reviews_cleaned_sample.csv\", converters={'tokenized': pd.eval})\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review = reviews_df.tokenized[0]\n",
    "type(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n",
      "i\n",
      "and\n",
      "a\n",
      ",\n",
      "the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(config.PREVIOUS_WORDS_CONSIDERED, len(sample_review)):\n",
    "    tokens = sample_review[index-config.PREVIOUS_WORDS_CONSIDERED:index]\n",
    "    \n",
    "    not_belonging = belong_to_vocabulary(tokens, vocabulary)\n",
    "\n",
    "    if not_belonging is None:\n",
    "        y_pred = model.predict(vectorize(test_tokens, word2vec), verbose=0)\n",
    "\n",
    "        predicted_indices = np.argsort(y_pred).reshape(len(vocabulary))[-num_proposed_word:]\n",
    "\n",
    "        for index in predicted_indices:\n",
    "            print(vocabulary[index])\n",
    "        print()\n",
    "        # if vocabulary[np.argmax(y_pred)] != \"the\":\n",
    "        #     print(tokens, vocabulary[np.argmax(y_pred)])\n",
    "        #     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a story about  i\n",
      "a story about  and\n",
      "a story about  a\n",
      "a story about  ,\n",
      "a story about  the\n"
     ]
    }
   ],
   "source": [
    "for predicted_index in predicted_indices:\n",
    "    print(\"a story about \", vocabulary[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1, 30).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
