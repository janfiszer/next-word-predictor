{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/word2vecs/pwc3-win7-vec200-min50/\"\n",
    "\n",
    "model = tf.keras.models.load_model(model_dir + \"predicor-extralayer-lr0.001-batch128.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_dir + \"vocabulary_small.pkl\", 'rb') as file:\n",
    "    vocabulary = pickle.load(file)\n",
    "\n",
    "with open(model_dir + \"word2vec.pkl\", 'rb') as file:\n",
    "    word2vec = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 200 * 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Returns the first not belonging token in the vocabulary\n",
    "\"\"\"\n",
    "def belong_to_vocabulary(tokens: list, vocabulary: list):\n",
    "    for token in tokens:\n",
    "        if not token in vocabulary:\n",
    "            return token\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens: list, word2vec):\n",
    "    embedded_tokens = np.array([])\n",
    "    \n",
    "    for token in tokens:\n",
    "        embedded_tokens = np.append(embedded_tokens, word2vec.wv.get_vector(token))\n",
    "\n",
    "    return embedded_tokens.reshape(1, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "marvelous\n"
     ]
    }
   ],
   "source": [
    "# quick test\n",
    "\n",
    "print(belong_to_vocabulary(['a', 'story', 'the'], vocabulary))\n",
    "print(belong_to_vocabulary(['a', 'story', 'marvelous', 'the'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8287111 , 0.5412161 , 0.20005795, 0.10574338, 0.35401824,\n",
       "        0.45367306, 0.09602261, 0.22088313, 0.40025556, 0.5392873 ,\n",
       "        0.5635784 , 0.43980128, 0.23014396, 0.5074685 , 0.16855712,\n",
       "        0.08047201, 0.22627427, 0.08021531, 0.2636618 , 0.19950962,\n",
       "        0.31991604, 0.16677734, 0.09795438, 0.14400737, 0.12036452,\n",
       "        0.2531241 , 0.34096292, 0.6339765 , 0.09568626, 0.0693303 ,\n",
       "        0.09909873, 0.35216552, 0.11913209, 0.19364113, 0.15176456,\n",
       "        0.17687114, 0.34218678, 0.30802083, 0.07646267, 0.1413412 ,\n",
       "        0.09426312, 0.09740797, 0.02743375, 0.04345704, 0.04709029,\n",
       "        0.1244511 , 0.6080524 , 0.45993158, 0.32360363, 0.08730534,\n",
       "        0.09894819, 0.19628564, 0.06592494, 0.20557976, 0.09030134,\n",
       "        0.09901462, 0.22819465, 0.11821279, 0.1211023 , 0.04136219,\n",
       "        0.101469  , 0.41000432, 0.07166714, 0.0555346 , 0.09031345,\n",
       "        0.09812423, 0.06894501, 0.08771562, 0.06199967, 0.21627213,\n",
       "        0.22747362, 0.13074657, 0.14340149, 0.2045555 , 0.09461729,\n",
       "        0.04674944, 0.2541387 , 0.24456081, 0.22002125, 0.1545371 ,\n",
       "        0.07599391, 0.08947273, 0.21063487, 0.07145577, 0.08324442,\n",
       "        0.12208504, 0.04045617, 0.03366439, 0.05349015, 0.05739377,\n",
       "        0.08374912, 0.06345091, 0.09110291, 0.05951613, 0.07726319,\n",
       "        0.18384476, 0.0442638 , 0.125522  , 0.1611556 , 0.12170935,\n",
       "        0.05834038, 0.12086618, 0.13623235, 0.16426213, 0.06580197,\n",
       "        0.07256609, 0.10447951, 0.09750745, 0.08713833, 0.25321728,\n",
       "        0.09732857, 0.03693437, 0.10090122, 0.06074728, 0.03607246,\n",
       "        0.07561748, 0.0340656 , 0.09132519, 0.06107694, 0.07538914,\n",
       "        0.03995988, 0.06448511, 0.04646   , 0.07150139, 0.04207261,\n",
       "        0.02756125, 0.05918294, 0.03885955, 0.02580849, 0.04797066,\n",
       "        0.16029085, 0.03545614, 0.135532  , 0.12870021, 0.07052337,\n",
       "        0.21969752, 0.319142  , 0.2625463 , 0.04348997, 0.09552092,\n",
       "        0.06413244, 0.09410714, 0.07712636, 0.08028828, 0.23656294,\n",
       "        0.13814436, 0.10684341, 0.08538292, 0.09373829, 0.08531515,\n",
       "        0.03226503, 0.07917313, 0.06941244, 0.06104911, 0.05907721,\n",
       "        0.05946758, 0.03121088, 0.05991096, 0.1217228 , 0.16790496,\n",
       "        0.08710824, 0.21097705, 0.07535759, 0.05911152, 0.08939496,\n",
       "        0.03927528, 0.09013508, 0.18900713, 0.06457919, 0.16345482,\n",
       "        0.07545815, 0.06125015, 0.05312403, 0.05744773, 0.18132728,\n",
       "        0.03719838, 0.16670011, 0.05145612, 0.05852051, 0.2509474 ,\n",
       "        0.09216017, 0.15066473, 0.11279832, 0.20476975, 0.11271366,\n",
       "        0.05571882, 0.04678477, 0.07843328, 0.10795239, 0.16529475,\n",
       "        0.1320064 , 0.07033124, 0.08029376, 0.06385412, 0.13476853,\n",
       "        0.23560873, 0.06322973, 0.17261784, 0.05355323, 0.17423922,\n",
       "        0.11719979, 0.05643499, 0.0951677 , 0.07382009, 0.05411417,\n",
       "        0.07689173, 0.17112611, 0.17062095, 0.11272979, 0.18752253,\n",
       "        0.08178581, 0.20613152, 0.04326331, 0.0578005 , 0.06314874,\n",
       "        0.09763078, 0.09484549, 0.04757318, 0.12278391, 0.07120115,\n",
       "        0.04487106, 0.05969448, 0.09340861, 0.05500137, 0.09868497,\n",
       "        0.07010377, 0.04095005, 0.13511753, 0.11325707, 0.09489061,\n",
       "        0.09391157, 0.08770809, 0.07514757, 0.07418068, 0.10269351,\n",
       "        0.09649594, 0.04859432, 0.03898256, 0.04049824, 0.06707311,\n",
       "        0.25935245, 0.10330857, 0.09975256, 0.04329926, 0.04387108,\n",
       "        0.06135639, 0.13622007, 0.03354934, 0.18607993, 0.3209142 ,\n",
       "        0.08428035, 0.06062637, 0.06050784, 0.09228078, 0.08199226,\n",
       "        0.05053855, 0.04447766, 0.19151525, 0.12256997, 0.06347607,\n",
       "        0.06839284, 0.04762166, 0.05712974, 0.05224583, 0.15984364,\n",
       "        0.05179346, 0.13581327, 0.06853947, 0.20349701, 0.04576159,\n",
       "        0.07130061, 0.16961016, 0.11568101, 0.07580187, 0.07042658,\n",
       "        0.21843721, 0.11829254, 0.06022766, 0.05398335, 0.03837625,\n",
       "        0.20102271, 0.06842661, 0.09652057, 0.0602359 , 0.02008802,\n",
       "        0.06657071, 0.03723987, 0.04013579, 0.10409264, 0.07090631,\n",
       "        0.09340905, 0.07475428, 0.04544754, 0.06198658, 0.03850863,\n",
       "        0.05958106, 0.07042654, 0.06688015, 0.03887289, 0.10745383,\n",
       "        0.15928902, 0.07234908, 0.12953007, 0.07061265, 0.05998509,\n",
       "        0.06031061, 0.07207843, 0.08192317, 0.08424635, 0.08717348,\n",
       "        0.05929167, 0.09712614, 0.05509213, 0.03993431, 0.09406665,\n",
       "        0.18075496, 0.12108728, 0.03191257, 0.10841454, 0.0367364 ,\n",
       "        0.09912586, 0.0928032 , 0.03278998, 0.05477205, 0.06497839,\n",
       "        0.05085037, 0.04116198, 0.07417779, 0.14835176, 0.07847294,\n",
       "        0.06911433, 0.07550094, 0.06344913, 0.04242028, 0.05130062,\n",
       "        0.0507343 , 0.0404816 , 0.12385322, 0.10098086, 0.09214465,\n",
       "        0.03758076, 0.05294897, 0.08094453, 0.04237394, 0.06798269,\n",
       "        0.10811389, 0.05201893, 0.08174907, 0.04785674, 0.06906164,\n",
       "        0.06382404, 0.06276563, 0.05401456, 0.04555219, 0.03988829,\n",
       "        0.05820884, 0.03550946, 0.1325161 , 0.06955183, 0.04399758,\n",
       "        0.04309731, 0.04233417, 0.11210243, 0.0551842 , 0.04760133,\n",
       "        0.06883688, 0.11267817, 0.02906146, 0.06400868, 0.03761432,\n",
       "        0.09783861, 0.05249485, 0.07303903, 0.05295797, 0.11604433,\n",
       "        0.08755899, 0.10122571, 0.04565107, 0.0795304 , 0.08956719,\n",
       "        0.12896867, 0.05081917, 0.14521016, 0.05493693, 0.06943651,\n",
       "        0.06190885, 0.04469051, 0.06363841, 0.08781381, 0.03827081,\n",
       "        0.09369529, 0.06309951, 0.12254367, 0.0697854 , 0.08094496,\n",
       "        0.0468808 , 0.036839  , 0.04477265, 0.03553601, 0.12170543,\n",
       "        0.04835031, 0.05038729, 0.05303614, 0.02380091, 0.06999495,\n",
       "        0.03474955, 0.08781318, 0.04994126, 0.09930864, 0.10578701,\n",
       "        0.05659109, 0.07837036, 0.09047594, 0.05389895, 0.060625  ,\n",
       "        0.05696132, 0.04747405, 0.04157119, 0.05084405, 0.05501893,\n",
       "        0.09541851, 0.03624593, 0.07780379, 0.0449072 , 0.05931363,\n",
       "        0.06282105, 0.04771528, 0.07261111, 0.03133629, 0.05935092,\n",
       "        0.09216938, 0.05608777, 0.08780815, 0.0668954 , 0.03863856,\n",
       "        0.03465267, 0.0615329 , 0.06720868, 0.07231672, 0.0312686 ,\n",
       "        0.22590078]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.randn(1, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = ['a', 'story', 'about']\n",
    "test_tokens = ['the', 'main', 'character']\n",
    "# test_tokens = ['my', 'feelings', 'were']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "not_belonging = belong_to_vocabulary(test_tokens, vocabulary)\n",
    "\n",
    "if not_belonging is None:\n",
    "    y_pred = model.predict(vectorize(test_tokens, word2vec))\n",
    "else:\n",
    "    print(\"ERROR: Word \\'{}\\' does not belong to the vocabulary\".format(not_belonging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_word = vocabulary[np.argmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proposed_word = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices = np.argsort(y_pred).reshape(len(vocabulary))[-num_proposed_word:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a story about  a\n",
      "a story about  of\n",
      "a story about  is\n",
      "a story about  and\n",
      "a story about  the\n"
     ]
    }
   ],
   "source": [
    "for predicted_index in predicted_indices:\n",
    "    print(\"a story about \", vocabulary[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flavia the Heretic is an undeniable work of ar...</td>\n",
       "      <td>False</td>\n",
       "      <td>flavia the heretic is an undeniable work of ar...</td>\n",
       "      <td>[flavia, the, heretic, is, an, undeniable, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROUEN PRIZES AND THE TRIUMPH OF \"VILLA PARANOI...</td>\n",
       "      <td>False</td>\n",
       "      <td>rouen prizes and the triumph of villa paranoia...</td>\n",
       "      <td>[rouen, prizes, and, the, triumph, of, villa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I liked the movie, first of all because it tol...</td>\n",
       "      <td>False</td>\n",
       "      <td>i liked the movie first of all because it told...</td>\n",
       "      <td>[i, liked, the, movie, first, of, all, because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Im watching it now on pink (Serbia TV station)...</td>\n",
       "      <td>False</td>\n",
       "      <td>im watching it now on pink serbia tv station a...</td>\n",
       "      <td>[im, watching, it, now, on, pink, serbia, tv, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A warm, touching movie that has a fantasy-like...</td>\n",
       "      <td>False</td>\n",
       "      <td>a warm touching movie that has a fantasylike q...</td>\n",
       "      <td>[a, warm, touching, movie, that, has, a, fanta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>I caught Evening in the cinema with a lady fri...</td>\n",
       "      <td>False</td>\n",
       "      <td>i caught evening in the cinema with a lady fri...</td>\n",
       "      <td>[i, caught, evening, in, the, cinema, with, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>I originally scored Sarah's show with a nice f...</td>\n",
       "      <td>False</td>\n",
       "      <td>i originally scored sarahs show with a nice fa...</td>\n",
       "      <td>[i, originally, scored, sarahs, show, with, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>Users who have rated this movie so highly simp...</td>\n",
       "      <td>False</td>\n",
       "      <td>users who have rated this movie so highly simp...</td>\n",
       "      <td>[users, who, have, rated, this, movie, so, hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>This is an exquisite film about the search for...</td>\n",
       "      <td>False</td>\n",
       "      <td>this is an exquisite film about the search for...</td>\n",
       "      <td>[this, is, an, exquisite, film, about, the, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>By God, it's been a long time since I saw this...</td>\n",
       "      <td>False</td>\n",
       "      <td>by god its been a long time since i saw this p...</td>\n",
       "      <td>[by, god, its, been, a, long, time, since, i, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  positive  \\\n",
       "0     Flavia the Heretic is an undeniable work of ar...     False   \n",
       "1     ROUEN PRIZES AND THE TRIUMPH OF \"VILLA PARANOI...     False   \n",
       "2     I liked the movie, first of all because it tol...     False   \n",
       "3     Im watching it now on pink (Serbia TV station)...     False   \n",
       "4     A warm, touching movie that has a fantasy-like...     False   \n",
       "...                                                 ...       ...   \n",
       "1495  I caught Evening in the cinema with a lady fri...     False   \n",
       "1496  I originally scored Sarah's show with a nice f...     False   \n",
       "1497  Users who have rated this movie so highly simp...     False   \n",
       "1498  This is an exquisite film about the search for...     False   \n",
       "1499  By God, it's been a long time since I saw this...     False   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     flavia the heretic is an undeniable work of ar...   \n",
       "1     rouen prizes and the triumph of villa paranoia...   \n",
       "2     i liked the movie first of all because it told...   \n",
       "3     im watching it now on pink serbia tv station a...   \n",
       "4     a warm touching movie that has a fantasylike q...   \n",
       "...                                                 ...   \n",
       "1495  i caught evening in the cinema with a lady fri...   \n",
       "1496  i originally scored sarahs show with a nice fa...   \n",
       "1497  users who have rated this movie so highly simp...   \n",
       "1498  this is an exquisite film about the search for...   \n",
       "1499  by god its been a long time since i saw this p...   \n",
       "\n",
       "                                              tokenized  \n",
       "0     [flavia, the, heretic, is, an, undeniable, wor...  \n",
       "1     [rouen, prizes, and, the, triumph, of, villa, ...  \n",
       "2     [i, liked, the, movie, first, of, all, because...  \n",
       "3     [im, watching, it, now, on, pink, serbia, tv, ...  \n",
       "4     [a, warm, touching, movie, that, has, a, fanta...  \n",
       "...                                                 ...  \n",
       "1495  [i, caught, evening, in, the, cinema, with, a,...  \n",
       "1496  [i, originally, scored, sarahs, show, with, a,...  \n",
       "1497  [users, who, have, rated, this, movie, so, hig...  \n",
       "1498  [this, is, an, exquisite, film, about, the, se...  \n",
       "1499  [by, god, its, been, a, long, time, since, i, ...  \n",
       "\n",
       "[1500 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"data/reviews_cleaned_sample.csv\", converters={'tokenized': pd.eval})\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review = reviews_df.tokenized[1]\n",
    "type(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwc_and_predicted = []\n",
    "\n",
    "for index in range(config.PREVIOUS_WORDS_CONSIDERED, len(sample_review)):\n",
    "    tokens = sample_review[index-config.PREVIOUS_WORDS_CONSIDERED:index]\n",
    "    \n",
    "    not_belonging = belong_to_vocabulary(tokens, vocabulary)\n",
    "\n",
    "    if not_belonging is None:\n",
    "        y_pred = model.predict(vectorize(tokens, word2vec), verbose=0)\n",
    "\n",
    "        predicted_indices = np.argsort(y_pred).reshape(len(vocabulary))[-num_proposed_word:]\n",
    "        probabilities = np.sort(y_pred).reshape(len(vocabulary))[-num_proposed_word:]\n",
    "\n",
    "        predicted_probability = {vocabulary[index]: prob for index, prob in zip(predicted_indices, probabilities)}\n",
    "        \n",
    "        pwc_and_predicted.append((tokens, predicted_probability))\n",
    "        # if vocabulary[np.argmax(y_pred)] != \"the\":\n",
    "        #     pwc_and_predicted.append((tokens, vocabulary[np.argmax(y_pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the', 'favorite', 'film'],\n",
       "  {'i': 0.5995596,\n",
       "   'the': 0.6090629,\n",
       "   'that': 0.6522759,\n",
       "   'is': 0.67576826,\n",
       "   'and': 0.7038415}),\n",
       " (['favorite', 'film', 'of'],\n",
       "  {'and': 0.5412394,\n",
       "   'this': 0.5544267,\n",
       "   'his': 0.5647545,\n",
       "   'a': 0.5832474,\n",
       "   'the': 0.93613875}),\n",
       " (['film', 'of', 'the'],\n",
       "  {'best': 0.7498231,\n",
       "   'most': 0.763092,\n",
       "   'worst': 0.77621627,\n",
       "   'film': 0.79021233,\n",
       "   'movie': 0.8514735}),\n",
       " (['which', 'was', 'also'],\n",
       "  {'to': 0.5856237,\n",
       "   'in': 0.6298851,\n",
       "   'and': 0.64227915,\n",
       "   'the': 0.7358801,\n",
       "   'a': 0.7587391}),\n",
       " (['up', 'three', 'more'],\n",
       "  {'the': 0.5864205,\n",
       "   'to': 0.660791,\n",
       "   'and': 0.718182,\n",
       "   'of': 0.72854763,\n",
       "   'than': 0.88650584}),\n",
       " (['best', 'film', 'audience'],\n",
       "  {'a': 0.56604636,\n",
       "   'of': 0.61137503,\n",
       "   'in': 0.6415446,\n",
       "   'the': 0.7455796,\n",
       "   'and': 0.80563545}),\n",
       " (['and', 'best', 'film'],\n",
       "  {'i': 0.6144221,\n",
       "   'the': 0.61739063,\n",
       "   'that': 0.679921,\n",
       "   'is': 0.69340104,\n",
       "   'and': 0.70720917}),\n",
       " (['best', 'film', 'of'],\n",
       "  {'his': 0.50653636,\n",
       "   'them': 0.5588597,\n",
       "   'those': 0.5899572,\n",
       "   'my': 0.63974386,\n",
       "   'the': 0.94026566}),\n",
       " (['film', 'of', 'another'],\n",
       "  {'a': 0.7324342,\n",
       "   'the': 0.7659204,\n",
       "   'in': 0.77091885,\n",
       "   'to': 0.79471,\n",
       "   'and': 0.84327}),\n",
       " (['not', 'a', 'bad'],\n",
       "  {'guy': 0.54833037,\n",
       "   'thing': 0.5790855,\n",
       "   'of': 0.5844217,\n",
       "   'film': 0.6771107,\n",
       "   'movie': 0.8429118}),\n",
       " (['for', 'a', 'film'],\n",
       "  {'[END]': 0.37519515,\n",
       "   'the': 0.37674612,\n",
       "   'with': 0.39336133,\n",
       "   'and': 0.5208175,\n",
       "   'that': 0.66494346}),\n",
       " (['a', 'film', 'from'],\n",
       "  {'to': 0.521627,\n",
       "   'this': 0.5623309,\n",
       "   'and': 0.6605267,\n",
       "   'a': 0.80715036,\n",
       "   'the': 0.9273754}),\n",
       " (['film', 'from', 'a'],\n",
       "  {'movie': 0.7100955,\n",
       "   'in': 0.71391654,\n",
       "   'to': 0.73271275,\n",
       "   'and': 0.843655,\n",
       "   'of': 0.88541484}),\n",
       " (['from', 'a', 'small'],\n",
       "  {'to': 0.6322798,\n",
       "   'the': 0.67174554,\n",
       "   'in': 0.7263504,\n",
       "   'of': 0.81983906,\n",
       "   'and': 0.8300813}),\n",
       " (['making', 'for', 'a'],\n",
       "  {'lot': 0.7150209,\n",
       "   'movie': 0.7612316,\n",
       "   'film': 0.7625898,\n",
       "   'and': 0.80935746,\n",
       "   'of': 0.87525934}),\n",
       " (['years', 'there', 'is'],\n",
       "  {'so': 0.58287877,\n",
       "   'and': 0.59097236,\n",
       "   'the': 0.6724491,\n",
       "   'no': 0.74921924,\n",
       "   'a': 0.81956446}),\n",
       " (['there', 'is', 'now'],\n",
       "  {'i': 0.419456,\n",
       "   'on': 0.4230587,\n",
       "   'the': 0.4294426,\n",
       "   'and': 0.48059258,\n",
       "   'to': 0.5626731}),\n",
       " (['is', 'now', 'a'],\n",
       "  {'movie': 0.7432335,\n",
       "   'good': 0.75546634,\n",
       "   'film': 0.7718734,\n",
       "   'and': 0.7959924,\n",
       "   'of': 0.8548327}),\n",
       " (['now', 'a', 'good'],\n",
       "  {'time': 0.60094595,\n",
       "   'job': 0.61812764,\n",
       "   'of': 0.6886844,\n",
       "   'film': 0.703553,\n",
       "   'movie': 0.8454691}),\n",
       " (['of', 'actors', 'will'],\n",
       "  {'a': 0.4565972,\n",
       "   'have': 0.5396942,\n",
       "   'the': 0.5404458,\n",
       "   'not': 0.5963032,\n",
       "   'be': 0.8361903}),\n",
       " (['show', 'the', 'world'],\n",
       "  {'to': 0.42153925,\n",
       "   'in': 0.44710445,\n",
       "   'the': 0.5055489,\n",
       "   'and': 0.5707901,\n",
       "   'of': 0.7740045}),\n",
       " (['the', 'world', 'that'],\n",
       "  {'this': 0.7162193,\n",
       "   'it': 0.7850967,\n",
       "   'i': 0.8024152,\n",
       "   'is': 0.8193867,\n",
       "   'the': 0.8735063}),\n",
       " (['has', 'more', 'to'],\n",
       "  {'and': 0.6550123,\n",
       "   'see': 0.7480694,\n",
       "   'a': 0.7718288,\n",
       "   'be': 0.8870662,\n",
       "   'the': 0.9032121}),\n",
       " (['which', 'is', 'to'],\n",
       "  {'see': 0.6853331,\n",
       "   'and': 0.68621296,\n",
       "   'a': 0.8075689,\n",
       "   'be': 0.8804807,\n",
       "   'the': 0.9184227}),\n",
       " (['is', 'to', 'say'],\n",
       "  {'i': 0.1873768,\n",
       "   'about': 0.19201346,\n",
       "   'it': 0.22046196,\n",
       "   'the': 0.31762418,\n",
       "   'that': 0.60728216}),\n",
       " (['to', 'say', 'a'],\n",
       "  {'film': 0.74574876,\n",
       "   'good': 0.7603337,\n",
       "   'lot': 0.77175885,\n",
       "   'movie': 0.7860646,\n",
       "   'of': 0.80184567}),\n",
       " (['for', 'young', 'and'],\n",
       "  {'that': 0.68436205,\n",
       "   'and': 0.698747,\n",
       "   'i': 0.7338193,\n",
       "   'a': 0.8242439,\n",
       "   'the': 0.9000074}),\n",
       " (['young', 'and', 'old'],\n",
       "  {'i': 0.73847497,\n",
       "   'a': 0.7596975,\n",
       "   'in': 0.7952736,\n",
       "   'and': 0.8170191,\n",
       "   'the': 0.8376329}),\n",
       " (['of', 'his', 'film'],\n",
       "  {'i': 0.58640474,\n",
       "   'the': 0.6255222,\n",
       "   'that': 0.6642016,\n",
       "   'is': 0.6704844,\n",
       "   'and': 0.7196131}),\n",
       " (['has', 'such', 'a'],\n",
       "  {'to': 0.7236326,\n",
       "   'movie': 0.7388857,\n",
       "   'film': 0.7700675,\n",
       "   'and': 0.8167373,\n",
       "   'of': 0.86933565}),\n",
       " (['that', 'with', 'a'],\n",
       "  {'in': 0.7157776,\n",
       "   'movie': 0.72239447,\n",
       "   'to': 0.7303139,\n",
       "   'and': 0.8393215,\n",
       "   'of': 0.88091326}),\n",
       " (['with', 'a', 'little'],\n",
       "  {'bit': 0.6210481,\n",
       "   'and': 0.64663273,\n",
       "   'but': 0.6478305,\n",
       "   'of': 0.73573744,\n",
       "   'more': 0.7545002}),\n",
       " (['a', 'little', 'more'],\n",
       "  {'the': 0.5744326,\n",
       "   'to': 0.6495285,\n",
       "   'and': 0.7116126,\n",
       "   'of': 0.73019713,\n",
       "   'than': 0.8931818}),\n",
       " (['in', 'by', 'mr'],\n",
       "  {'to': 0.711668,\n",
       "   'in': 0.77411926,\n",
       "   'a': 0.7989168,\n",
       "   'and': 0.8289273,\n",
       "   'the': 0.8330857}),\n",
       " (['young', 'actress', 'has'],\n",
       "  {'some': 0.6392182,\n",
       "   'the': 0.7337962,\n",
       "   'been': 0.8028735,\n",
       "   'to': 0.8117044,\n",
       "   'a': 0.8652265}),\n",
       " (['actress', 'has', 'lost'],\n",
       "  {'to': 0.6780429,\n",
       "   'a': 0.70070183,\n",
       "   'in': 0.77263814,\n",
       "   'the': 0.7792977,\n",
       "   'and': 0.8082553}),\n",
       " (['has', 'lost', 'a'],\n",
       "  {'movie': 0.72936666,\n",
       "   'to': 0.73479754,\n",
       "   'film': 0.75853395,\n",
       "   'and': 0.82741225,\n",
       "   'of': 0.8820337}),\n",
       " (['role', 'in', 'the'],\n",
       "  {'film': 0.71487427,\n",
       "   'first': 0.7263985,\n",
       "   'movie': 0.76518583,\n",
       "   'and': 0.85060817,\n",
       "   'of': 0.90973043}),\n",
       " (['to', 'making', 'an'],\n",
       "  {'hour': 0.67474747,\n",
       "   'in': 0.7142797,\n",
       "   'to': 0.737419,\n",
       "   'of': 0.7474781,\n",
       "   'and': 0.793606}),\n",
       " (['is', 'on', 'the'],\n",
       "  {'is': 0.71731377,\n",
       "   'movie': 0.7481353,\n",
       "   'film': 0.7532585,\n",
       "   'and': 0.8405419,\n",
       "   'of': 0.9099661}),\n",
       " (['her', 'a', 'job'],\n",
       "  {'in': 0.6006313,\n",
       "   'is': 0.6283451,\n",
       "   'who': 0.64081246,\n",
       "   'of': 0.710678,\n",
       "   'and': 0.71552044}),\n",
       " (['a', 'job', 'with'],\n",
       "  {'her': 0.5281913,\n",
       "   'this': 0.61381114,\n",
       "   'and': 0.7549188,\n",
       "   'a': 0.8599499,\n",
       "   'the': 0.892896}),\n",
       " (['care', 'of', 'his'],\n",
       "  {'in': 0.61989933,\n",
       "   'to': 0.6471477,\n",
       "   'the': 0.6867835,\n",
       "   'of': 0.7017578,\n",
       "   'and': 0.793224}),\n",
       " (['who', 'has', 'not'],\n",
       "  {'be': 0.5767537,\n",
       "   'that': 0.6185068,\n",
       "   'a': 0.7016824,\n",
       "   'to': 0.74290013,\n",
       "   'the': 0.77620006}),\n",
       " (['since', 'his', 'wife'],\n",
       "  {'a': 0.5556518,\n",
       "   'the': 0.6353496,\n",
       "   'to': 0.6656282,\n",
       "   'in': 0.67394996,\n",
       "   'and': 0.7042102}),\n",
       " (['is', 'the', 'only'],\n",
       "  {'that': 0.47092703,\n",
       "   'reason': 0.49873537,\n",
       "   'good': 0.5554205,\n",
       "   'one': 0.5906298,\n",
       "   'thing': 0.63192016}),\n",
       " (['the', 'only', 'one'],\n",
       "  {'i': 0.6076265,\n",
       "   'and': 0.63443184,\n",
       "   'is': 0.6370159,\n",
       "   'the': 0.6395031,\n",
       "   'of': 0.9043633}),\n",
       " (['only', 'one', 'who'],\n",
       "  {'was': 0.50335705,\n",
       "   'the': 0.52334464,\n",
       "   'have': 0.5541805,\n",
       "   'is': 0.5735314,\n",
       "   'are': 0.62519884}),\n",
       " (['a', 'way', 'of'],\n",
       "  {'his': 0.57580495,\n",
       "   'this': 0.65694004,\n",
       "   'a': 0.795198,\n",
       "   'and': 0.82138014,\n",
       "   'the': 0.9094066}),\n",
       " (['that', 'he', 'has'],\n",
       "  {'his': 0.5150741,\n",
       "   'the': 0.6787988,\n",
       "   'been': 0.71221733,\n",
       "   'to': 0.7904993,\n",
       "   'a': 0.84440464}),\n",
       " (['he', 'has', 'been'],\n",
       "  {'on': 0.5900584,\n",
       "   'for': 0.5972753,\n",
       "   'the': 0.7320833,\n",
       "   'to': 0.74717015,\n",
       "   'a': 0.85433453}),\n",
       " (['all', 'these', 'years'],\n",
       "  {'and': 0.57289064,\n",
       "   'the': 0.5923922,\n",
       "   'old': 0.6298346,\n",
       "   'later': 0.66784126,\n",
       "   'ago': 0.8712498}),\n",
       " (['these', 'years', 'a'],\n",
       "  {'to': 0.7156969,\n",
       "   'movie': 0.7445751,\n",
       "   'film': 0.7500491,\n",
       "   'and': 0.8174412,\n",
       "   'of': 0.8797698}),\n",
       " (['to', 'her', 'playing'],\n",
       "  {'a': 0.5610258,\n",
       "   'to': 0.65986526,\n",
       "   'the': 0.6630899,\n",
       "   'in': 0.69003505,\n",
       "   'and': 0.7245303}),\n",
       " (['her', 'playing', 'the'],\n",
       "  {'is': 0.72759825,\n",
       "   'film': 0.7516514,\n",
       "   'movie': 0.7585768,\n",
       "   'and': 0.85387516,\n",
       "   'of': 0.91128135}),\n",
       " (['role', 'of', 'her'],\n",
       "  {'the': 0.58708495,\n",
       "   'is': 0.63584805,\n",
       "   'in': 0.65358925,\n",
       "   'to': 0.70812106,\n",
       "   'and': 0.7818127}),\n",
       " (['of', 'her', 'own'],\n",
       "  {'to': 0.57843286,\n",
       "   'in': 0.6241481,\n",
       "   'a': 0.65618503,\n",
       "   'and': 0.69422525,\n",
       "   'the': 0.737623}),\n",
       " (['her', 'own', 'life'],\n",
       "  {'to': 0.6369491,\n",
       "   'is': 0.6415282,\n",
       "   'of': 0.6516354,\n",
       "   'in': 0.68645537,\n",
       "   'and': 0.7735601}),\n",
       " (['own', 'life', 'in'],\n",
       "  {'to': 0.56133467,\n",
       "   'and': 0.66057503,\n",
       "   'this': 0.7520454,\n",
       "   'a': 0.8297684,\n",
       "   'the': 0.9217689}),\n",
       " (['from', 'life', 'and'],\n",
       "  {'it': 0.6927065,\n",
       "   'i': 0.7213533,\n",
       "   'of': 0.7305802,\n",
       "   'a': 0.80691683,\n",
       "   'the': 0.90056545}),\n",
       " (['true', 'to', 'the'],\n",
       "  {'movie': 0.6849814,\n",
       "   'the': 0.71199995,\n",
       "   'to': 0.71415967,\n",
       "   'and': 0.8207498,\n",
       "   'of': 0.91203916}),\n",
       " (['from', 'which', 'it'],\n",
       "  {'would': 0.5856163,\n",
       "   'to': 0.62301826,\n",
       "   'has': 0.7196888,\n",
       "   'was': 0.90526307,\n",
       "   'is': 0.9150563}),\n",
       " (['which', 'it', 'is'],\n",
       "  {'and': 0.60335803,\n",
       "   'that': 0.6417497,\n",
       "   'not': 0.718515,\n",
       "   'the': 0.82145965,\n",
       "   'a': 0.8909016}),\n",
       " (['has', 'something', 'for'],\n",
       "  {'and': 0.6044689,\n",
       "   'me': 0.63614976,\n",
       "   'this': 0.6539852,\n",
       "   'a': 0.8287123,\n",
       "   'the': 0.89850086}),\n",
       " (['the', 'kind', 'of'],\n",
       "  {'that': 0.57587945,\n",
       "   'this': 0.7043901,\n",
       "   'a': 0.74401575,\n",
       "   'and': 0.7709125,\n",
       "   'the': 0.91726595})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwc_and_predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
